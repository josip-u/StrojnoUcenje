{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sveučilište u Zagrebu<br>\n",
    "Fakultet elektrotehnike i računarstva\n",
    "\n",
    "# Strojno učenje\n",
    "\n",
    "<a href=\"http://www.fer.unizg.hr/predmet/su\">http://www.fer.unizg.hr/predmet/su</a>\n",
    "\n",
    "Ak. god. 2015./2016.\n",
    "\n",
    "# Bilježnica 8: Stroj potpornih vektora (SVM)\n",
    "\n",
    "(c) 2015 Jan Šnajder\n",
    "\n",
    "<i>Verzija: 0.1 (2015-12-08)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sadržaj:\n",
    "\n",
    "* Uvod\n",
    "\n",
    "* Problem maksimalne margine\n",
    "\n",
    "* Metoda Lagrangeovih multiplikatora\n",
    "\n",
    "* Dualna formulacija problema maksimalne margine\n",
    "\n",
    "* Meka margina\n",
    "\n",
    "* Gubitak zglobnice\n",
    "\n",
    "* Jezgreni trik\n",
    "\n",
    "* Mercerove jezgre\n",
    "\n",
    "* Optimizacija hiperparametara\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uvod\n",
    "\n",
    "\n",
    "* Vrlo učinkovit **diskriminativan model**\n",
    "\n",
    "\n",
    "* Podsjetnik: Algoritam strojnog učenja definiran je \n",
    "  1. modelom\n",
    "  * pogreškom\n",
    "  * optimizacijskim postupkom\n",
    "    \n",
    "    \n",
    "* Model:\n",
    "    \n",
    "$$\n",
    "    h(\\mathbf{x}) = \\mathbf{w}^\\intercal\\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "\n",
    "* Dakle, to je poopćeni linearan model bez funkcije preslikavanja\n",
    "\n",
    "\n",
    "* Gornja definicija modela je tzv. **primarna formulacija**\n",
    "\n",
    "\n",
    "* Postoji i **dualna formulacija**:\n",
    "  * Umjesto da značajke novog primjera množimo težinama $\\mathbf{w}$, možemo za novi primjer izračunati koliko je sličan primjerima iz skupa za učenje i na temelju toga odrediti klasifikaciju\n",
    "  * Model efektivno postaje **neparametarski**!\n",
    "  \n",
    "  \n",
    "* U dualnoj formulaciji možemo iskoristiti tzv. **jezgreni trik**, koji nam omogućava jeftino preslikavanje primjera u prostor više dimenzije (a time i nelinearnost modela)\n",
    "  * SVM je jedna vrsta tzv. **jezgrenog stroja** (engl. *kernel machine*)\n",
    "\n",
    "\n",
    "* Model je jednostavan, no definicija pogreške i optimizacijskog postupka su nešto složeniji\n",
    "\n",
    "\n",
    "* Osnovna ideja: primjere dviju klasa razdvojiti tako da je prostor između njih što veći $\\Rightarrow$ tzv. **maksimalna margina**\n",
    "\n",
    "\n",
    "* Opravdanje: generalizacija će biti najbolja onda kada granicu između klasa povućemo točno po sredini margine\n",
    "\n",
    "\n",
    "* Do sada smo optimizacijski problem definirali tako da smo\n",
    "  * krenuli od log-izglednosti pa izveli MLE (Naivan bayes)\n",
    "  * krenuli od funkcije gubitka pa izveli funkciju pogreške (linearna regresija) i napravili analitičku minimizaciju\n",
    "  * krenuli od funkcije pogreške pa izveli funkciju gubitka (logistička regresija, perceptron) i iskoristili je za gradijentni spust\n",
    "  \n",
    "  \n",
    "* Kod SVM-a, krenut ćemo odmah od onoga što u konačnici želimo dobiti: maksimalnu marginu $\\Rightarrow$ to je ono što SVM optimizira\n",
    "\n",
    "\n",
    "* Ići ćemo \"klasičnim pristupom\": formalizirati to kao problem **kvadratnog programiranja**\n",
    "\n",
    "\n",
    "* Naknadno ćemo iz toga izvesti funkciju gubitka (i funkciju pogreške), ali samo radi usporedbe s drugim algoritmima\n",
    "\n",
    "\n",
    "> **Sinopsis:**\n",
    "> \n",
    "> * Prvo ćemo se fokusirati na **linearan model** i **linearno odvojive probleme** (tvrda granica)\n",
    "  * Matematika: Lagrangeovi multiplikatori\n",
    "> \n",
    "> * Zatim ćemo proširiti **linearan model** tako da može raditi s **linearno neodvojivim problemima** (meka granica)\n",
    "> \n",
    "> * Na kraju ćemo proširiti na **nelinearan model** (jezgreni trik)\n",
    ">   * Matematika: Mercerove jezgre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem maksimalne margine\n",
    "\n",
    "* Model:\n",
    "$$\n",
    "h(\\mathbf{x}) = \\mathbf{w}^\\intercal x + w_0\n",
    "$$\n",
    "\n",
    "\n",
    "* Oznake primjera za učenje: $y\\in\\{-1,+1\\}$\n",
    "\n",
    "\n",
    "* Granica između klasa: hiperravnina $h(\\mathbf{x})=0$\n",
    "\n",
    "\n",
    "* Predikcija klase: $y=\\mathrm{sgn}(h(\\mathbf{x}))$\n",
    "\n",
    "\n",
    "* Pretpostavimo da su primjeri iz $\\mathcal{D}$ **linearno odvojivi**\n",
    "\n",
    "\n",
    "* Onda postoji $\\mathbf{w}$ i $w_0$ takvi da\n",
    "$$\n",
    "\\begin{align*}\n",
    "h(\\mathbf{x}^{(i)}) \\geq 0 & \\quad\\text{za svaki $y^{(i)}=+1$}\\\\\n",
    "h(\\mathbf{x}^{(i)}) < 0 & \\quad\\text{za svaki $y^{(i)}=-1$}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* Kraće, postoje $\\mathbf{w}$ i $w_0$ takvi da\n",
    "$$\n",
    "\\forall(\\mathbf{x}^{(i)},y^{(i)}) \\in \\mathcal{D}.\\ y^{(i)}h(\\mathbf{x}^{(i)})\\geq 0\n",
    "$$\n",
    "\n",
    "\n",
    "* Postoji beskonačno mnogo rješenja za $\\mathbf{w}$ i $w_0$ (prostor inačica je beskonačan)\n",
    "\n",
    "\n",
    "* No nas zanima rješenje **maksimalne margine** $\\Rightarrow$ induktivna pristranost preferencijom\n",
    "\n",
    "\n",
    "* **Margina = udaljenost hiperravnine do najbližeg primjera**\n",
    "\n",
    "\n",
    "* Ako maksimiziramo marginu, onda će hiperravnina prolaziti točno na pola puta između dva primjera\n",
    "\n",
    "\n",
    "#### Formulacija optimizacijskog problema\n",
    "\n",
    "\n",
    "* Predznačena udaljenost primjera od hiperravnine je\n",
    "\n",
    "$$\n",
    "d = \\frac{h(\\mathbf{x})}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "\n",
    "\n",
    "* Nas zanimaju samo hiperravnine koje ispravno klasificiraju primjere. U tom slučaju **apsolutna** udaljenost primjera do hiperravnine je:\n",
    "\n",
    "$$\n",
    "\\frac{y^{(i)}h(\\mathbf{x})}{\\|\\mathbf{w}\\|} = \n",
    "\\frac{y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}^{(i)}+w_0)}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "\n",
    "\n",
    "* Po definiciji, margina je udaljenost hiperravnine do najbližeg primjera:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\|\\mathbf{w}\\|}\\mathrm{min}_i\\big\\{y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x} + w_0)\\big\\}\n",
    "$$\n",
    "\n",
    "\n",
    "* Tu udaljenost želimo maksimizirati:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmax}_{\\mathbf{w},w_0}\\Big\\{\\frac{1}{\\|\\mathbf{w}\\|}\\mathrm{min}_i\\big\\{y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x} + w_0)\\big\\}\\Big\\}\n",
    "$$\n",
    "\n",
    "\n",
    "* Ako je $\\mathcal{D}$ linearno odvojiv, onda postoji samo jedna takva margina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pojednostavljenje optimizacijskog problema\n",
    "\n",
    "\n",
    "* Gornji problem teško je riješiti izravno (min unutar max)\n",
    "\n",
    "\n",
    "* Vektor $(\\mathbf{w},w_0)$ možemo pomnožiti s proizvoljnom konstantom, a da to ne utječe na udaljenosti između primjera i hiperravnine:\n",
    "\n",
    "$$\n",
    "d=\\frac{h(\\mathbf{x})}{\\|\\mathbf{w}\\|}=\n",
    "\\frac{\\color{red}{\\alpha}\\mathbf{w}^\\intercal\\mathbf{x}+\\color{red}{\\alpha}w_0}{\\|\\color{red}{\\alpha}\\mathbf{w}\\|}=\n",
    "\\frac{\\color{red}{\\alpha}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0)}{\\color{red}{\\alpha}\\|\\mathbf{w}\\|}=\n",
    "\\frac{\\mathbf{w}^\\intercal\\mathbf{x}+w_0}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "\n",
    "\n",
    "* Kako bismo pojednostavili problem, možemo definirati da za primjer $\\mathbf{x}^{(i)}$ koji je najbliži margini vrijedi\n",
    "\n",
    "$$\n",
    "y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0)=1\n",
    "$$\n",
    "\n",
    "\n",
    "* Svi ostali primjeri jednako su blizu margine ili su od nje još udaljeniji:\n",
    "\n",
    "$$\n",
    "y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0) \\geq 1, \\qquad n=1,\\dots,N\n",
    "$$\n",
    "\n",
    "\n",
    "* Za primjere za koje $y^{(i)}h(\\mathbf{x})=1$ kažemo da su ograničenja **aktivna**, dok su za ostale primjere ograničenja neaktivna \n",
    "  * (Primjeri za koje su ograničenja aktivna zovemo potpornim vektorima, ali o tome više kasnije)\n",
    "\n",
    "\n",
    "* Uvijek će postojati barem **dva** aktivna ograničenja\n",
    "\n",
    "\n",
    "* [Skica: maksimalna margina]\n",
    "\n",
    "\n",
    "* Dakle, umjesto\n",
    "$$\n",
    "\\mathrm{argmax}_{\\mathbf{w},w_0}\\Big\\{\\frac{1}{\\|\\mathbf{w}\\|}\\underbrace{\\mathrm{min}_i\\big\\{y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x} + w_0)\\big\\}}_{=1}\\Big\\}\n",
    "$$\n",
    "mi sada maksimiziramo\n",
    "$$\n",
    "\\mathrm{argmax}_{\\mathbf{w},w_0}\\frac{1}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "uz ograničenja\n",
    "$$\n",
    "y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0) \\geq 1, \\qquad n=1,\\dots,N\n",
    "$$\n",
    "\n",
    "\n",
    "* Maksimizator od $\\frac{1}{\\|\\mathbf{w}\\|}$ ekvivalentan je minimizatoru od $\\|\\mathbf{w}\\|=\\sqrt{\\mathbf{w}^\\intercal\\mathbf{w}}$, a taj je ekvivalentan minimizatoru od $\\|\\mathbf{w}\\|^2$. Još ćemo pomnožiti s $\\frac{1}{2}$ radi kasnije matematičke jednostavnosti\n",
    "\n",
    "\n",
    "* Konačna formulacija optimizacijskog problema maksimalne margine:\n",
    "\n",
    "> $\\mathrm{argmin}_{\\mathbf{w},w_0}\\frac{1}{2}\\|\\mathbf{w}\\|^2$\n",
    "\n",
    "> tako da $\\quad y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0) \\geq 1, \\quad n=1,\\dots,N$\n",
    "\n",
    "\n",
    "* Naš optimizacijski problem sveo se na ciljnu funkciju koju želimo optimirati i ograničenja koja pritom moramo poštovati: tipičan problem **konveksne optimizacije uz ograničenja**, točnije **kvadratnog programiranja**\n",
    "  * \"Programiranje\" = matematičko programiranje = (matematička) optimizacija\n",
    "\n",
    "\n",
    "* Može se riješiti metodom **Lagrangeovih multiplikatora**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metoda Lagrangeovih multiplikatora\n",
    "\n",
    "#### Optimizacijski problemi\n",
    "\n",
    "* Optimizacijski problem $\\Rightarrow$ Optimizacija uz ograničenja $\\Rightarrow$ Konveksni optimizacijski problem $\\Rightarrow$ Kvadratno programiranje\n",
    "\n",
    "\n",
    "* Optimizacijski problem uz ograničenja (standardni oblik):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimizirati} &\\quad f(\\mathbf{x})\\\\\n",
    "\\text{uz ograničenja} &\\quad g_i(\\mathbf{x})\\leq0,\\quad i=1,\\dots,m\\\\\n",
    "&\\quad h_i(\\mathbf{x}) = 0,\\quad i=1,\\dots,p\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* $f : \\mathbb{R}^n\\to\\mathbb{R}$ je **ciljna funkcija** (engl. *objective function*\n",
    "* $h_i : \\mathbb{R}^n\\to\\mathbb{R}$ su **ograničenja\n",
    "jednakosti** (engl. *equality constraints*)\n",
    "* $g_i:\\mathbb{R}^n\\to\\mathbb{R}$ su **ograničenja nejednakosti** (engl. *inequality constraints*)\n",
    "\n",
    "\n",
    "* **NB:** Sva ograničenja (ne)jednakosti mogu se se svesti na standardni oblik\n",
    "\n",
    "\n",
    "* Tražimo minimum koji zadovoljava sva ograničenja\n",
    "* Točke koje zadovoljavaju rješenja zovemo **ostvarivim\n",
    "točkama** (engl. *feasible points*)\n",
    "\n",
    "\n",
    "* Konveksni optimizacijski problem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimizirati} &\\quad f(\\mathbf{x})\\ \\color{red}{\\text{$\\Rightarrow$ konveksna}}\\\\\n",
    "\\text{uz ograničenja} &\\quad g_i(\\mathbf{x})\\leq0,\\quad i=1,\\dots,m\\\\\n",
    "&\\quad \\mathbf{a}_i^\\intercal\\mathbf{x} - b_i = 0,\\quad i=1,\\dots,p\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Kvadratni program:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{minimizirati} &\\quad \\frac{1}{2}\\mathbf{x}^\\intercal P\\mathbf{x} + \\mathbf{q}^\\intercal\\mathbf{x} + r\\ \\color{red} {\\text{$\\Rightarrow$ kvadratna funkcija}}\\\\\n",
    "\\text{uz ograničenja} &\\quad G\\mathbf{x}\\leq\\mathbf{h}\\\\\n",
    "&\\quad A\\mathbf{x}=\\mathbf{b}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metoda Lagrangeovih multiplikatora\n",
    "\n",
    "* Kvadratni program može se riješiti metodom **Langrangeovih multiplikatora**\n",
    "  * Točnije: **proširen Lagrangeov postupak** (engl. *augmented Lagrangian method*), jer imamo ograničenja nejednakosti (izvorna metoda dopušta samo ograničenja jednakosti)\n",
    "\n",
    "\n",
    "* Lagrangeovi multiplikatori mogu se koristiti općenito za optimizaciju s ograničenjima (problem ne mora biti konveksan)\n",
    "\n",
    "\n",
    "* Pripada skupini **metoda s kaznom** (engl. *penalty methods*)\n",
    "\n",
    "\n",
    "* Ideja: umjesto ograničenog problema, riješiti niz neograničenih problema koji uključuju kaznu za kršenje ograničenja\n",
    "\n",
    "\n",
    "* Alternativa: **metode unutarnje točke** (engl. *interior-point methods*, *barrier methods*)\n",
    "\n",
    "#### Lagrangeova funkcija\n",
    "\n",
    "* **Lagrangeova funkcija** izravno ugrađuje ograničenja u ciljnu funkciju\n",
    "\n",
    "\n",
    "* [Skica ideje]\n",
    "\n",
    "\n",
    "* Početni problem:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{minimizirati} &\\quad f(\\mathbf{x})\\\\\n",
    "\\text{uz ograničenja} &\\quad g_i(\\mathbf{x})\\leq0,\\quad i=1,\\dots,m\\\\\n",
    " &\\quad h_i(\\mathbf{x}) = 0,\\quad i=1,\\dots,p\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "* Lagrangeova funkcija:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L(\\mathbf{x},\\color{red}{\\boldsymbol\\alpha},\\color{red}{\\boldsymbol\\beta}) = f(\\mathbf{x}) + \\sum_{i=1}^m\\color{red}{\\alpha_i} g(\\mathbf{x}) + \\sum_{i=1}^p\\color{red}{\\beta_i}\n",
    "h(\\mathbf{x})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "* Dobili smo neograničeni problem optimizacije: optimum Lagrangeove funkcije odgovara optimimu odgovarajućeg problema s ograničenjim\n",
    "\n",
    "\n",
    "* Vrijednosti $\\alpha_i$ i $\\beta_i$ su **Lagrangeovi multiplikatori** (množitelji) za ograničenja nejednakosti odnosno jednakosti\n",
    "\n",
    "\n",
    "* Za vrijednosti $\\alpha_i$ vrijede takozvani **Karush-Kuhn-Tuckerovi (KKT)** uvjeti:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_i &\\geq 0,\\quad i=1,\\dots,p\\\\\n",
    "\\alpha_i g_i(\\mathbf{x}) &= 0,\\quad i=1,\\dots,p\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Lagrangeova dualnost\n",
    "\n",
    "\n",
    "* Načelo dualnosti u teoriji optimizacije:\n",
    "  * **Primarni problem** (engl. *primal problem*): minimizacija funkcije $f(\\mathbf{x})$\n",
    "  * **Dualni problem**: nalaženje donje granice primarnog problema (\"minumum ne može biti manji od $\\mathbf{x}$\")\n",
    "\n",
    "\n",
    "* [Skica: primal-dual sedlo]\n",
    "\n",
    "\n",
    "* Općenito, rješenja primarnog i dualnog problema se ne preklapaju već postoji **dualni procjep**\n",
    "\n",
    "\n",
    "* Uz određene uvjete, kod konveksne optimizacije dualni procjep jednak je nuli $\\Rightarrow$ **jaka dualnost**\n",
    "\n",
    "\n",
    "* To znači da je rješenje dualnog problema ujedno i rješenje primarnog problema\n",
    "\n",
    "\n",
    "* Dakle, ako nam je tako pogodnije, možemo rješavati dualni problem umjesto primarnog\n",
    "\n",
    "\n",
    "* U nastavku promatramo Lagrangeovu dualnost\n",
    "\n",
    "\n",
    "* Lagrangeova funkcija (NSO: samo s ograničenjima jednakosti):\n",
    "\n",
    "$$\n",
    "L(\\mathbf{x},\\boldsymbol\\alpha) = f(\\mathbf{x}) + \\sum_i\\alpha_i g_i(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "\n",
    "* To je fukcija od $\\mathbf{x}$ (**primarne varijable**) i $\\boldsymbol\\alpha$ (**dualne varijable**)\n",
    "\n",
    "\n",
    "* Optimum:\n",
    "$$\n",
    "L(\\mathbf{x}^*,\\boldsymbol\\alpha^*) = \\min_{\\mathbf{x},\\boldsymbol\\alpha} L(\\mathbf{x},\\boldsymbol\\alpha)\n",
    "$$\n",
    "\n",
    "\n",
    "* Vrijednost za $\\mathbf{x}^*$ nalazimo rješavanjem sustava:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nabla f(\\mathbf{x}) + \\nabla \\sum_i\\alpha_i h_i(\\mathbf{x}) = 0\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "* To rješenje ne mora dovesti do uklanjanja dualnih varijabli! Općenito, dobit ćemo rješenje koje mimizira $\\mathbf{x}$ za neki zadani $\\boldsymbol\\alpha$:\n",
    "$$\n",
    "\\tilde{L}(\\boldsymbol\\alpha)= \\min_{\\mathbf{x}}L(\\mathbf{x},\\boldsymbol\\alpha) = \\min_{\\mathbf{x}}\\Big(f(\\mathbf{x}) + \\sum_i\n",
    "\\alpha_i g(\\mathbf{x})\\Big)\n",
    "$$\n",
    "$\\Rightarrow$ **Dualna Lagrangeova funkcija**\n",
    "\n",
    "\n",
    "* Sigurno vrijedi:\n",
    "$$\n",
    "\\tilde{L}(\\boldsymbol\\alpha) \\leq L(\\mathbf{x}^*,\\boldsymbol\\alpha)\n",
    "$$\n",
    "$\\Rightarrow$ Dualna Lagrangeova funkcija je **donja ograda** primarnog problema\n",
    "\n",
    "\n",
    "* U točki $\\boldsymbol\\alpha^*$ vrijedi $\\tilde{L}(\\boldsymbol\\alpha^*)=L(\\mathbf{x}^*,\\boldsymbol\\alpha^*)$\n",
    "\n",
    "\n",
    "* Kako bismo pronašli $\\boldsymbol\\alpha^*$, moramo maksimizirati\n",
    "donju ogradu, tj. riješiti sljedeći konveksni problem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{maksimizirati} &\\quad \\tilde{L}(\\boldsymbol\\alpha)\\\\\n",
    "\\text{uz ograničenja} &\\quad \\alpha_i\\geq 0,\\quad i=1,\\dots,m\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* **NB:** minimizacija ciljne funkcije $\\Leftrightarrow$ maksimizacija dualne funkcije"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dualna formulacija problema maksimalne margine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Optimizacijski problem maksimalne margine:\n",
    "\n",
    "> $\\mathrm{argmin}_{\\mathbf{w},w_0}\\frac{1}{2}\\|\\mathbf{w}\\|^2$\n",
    "\n",
    "> tako da $\\quad y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0) \\geq 1, \\quad n=1,\\dots,N$\n",
    "\n",
    "\n",
    "* Odgovarajuća Lagrangeova funkcija:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w},w_0,\\color{red}{\\boldsymbol\\alpha})=\\frac{1}{2}\\|\\mathbf{w}\\|^2 -\n",
    "\\sum_{i=1}^N\\color{red}{\\alpha_i}\\Big\\{y^{(i)}\\big(\\mathbf{w}^\\intercal\\mathbf{x}^{(i)}+w_0\\big)-1\\Big\\}\n",
    "$$\n",
    "\n",
    "\n",
    "*  $\\color{red}{\\boldsymbol\\alpha=(\\alpha_1,\\dots,\\alpha_N)}$ je vektor Lagrangeovih multiplikatora, po jedan za svako ograničenje\n",
    "\n",
    "\n",
    "* Prelazimo na **dualnu formulaciju** problema jer je ta formulacija jednostavnija (optimirat ćemo samo po $\\boldsymbol\\alpha$, a postoje i neke druge prednosti)\n",
    "\n",
    "\n",
    "* Minimizator $(\\mathbf{w}^*, w_0^*)$: deriviranje po $\\mathbf{w}$ odnosno $w_0$ i izjednačavanje s nulom:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= \\sum_{i=1}^N \\alpha_i y^{(i)}\\mathbf{x}^{(i)}\\\\\n",
    "0 &= \\sum_{i=1}^N\\alpha_i y^{(i)}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dualna Lagrangeova funkcija:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{L}(\\boldsymbol\\alpha) &=\n",
    "\\frac{1}{2}\\|\\mathbf{w}\\|^2 -\\sum_{i=1}^N\\alpha_i\\Big\\{y^{(i)}\\big(\\mathbf{w}^\\intercal\\mathbf{x}^{(i)}+w_0\\big)-1\\Big\\}\\\\\n",
    "&= \n",
    "\\frac{1}{2}\\|\\mathbf{w}\\|^2\n",
    "-\\sum_{i=1}^N\\alpha_i y^{(i)}\\mathbf{w}^\\intercal\\mathbf{x}^{(i)}\n",
    "\\color{gray}{\\underbrace{-w_0 \\sum_{i=1}^N\\alpha_iy^{(i)}}_{=0}} + \n",
    "\\sum_{i=1}^N\\alpha_i\\\\\n",
    "&= \\nonumber\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\alpha_i y^{(i)}(\\mathbf{x}^{(i)})^\\intercal\\sum_{j=1}^N\\alpha_j y^{(j)}\\mathbf{x}^{(j)}\n",
    "-\n",
    "\\sum_{i=1}^N\\alpha_i y^{(i)}(\\mathbf{x}^{(i)})^\\intercal\\sum_{j=1}^N\\alpha_j y^{(j)}\\mathbf{x}^{(j)}\n",
    "+ \\sum_{i=1}^N\\alpha_i\\\\\n",
    "&= \n",
    "\\sum_{i=1}^N\\alpha_i - \n",
    "\\frac{1}{2}\\sum_{i=1}^N\n",
    "\\sum_{j=1}^N\n",
    "\\alpha_i\n",
    "\\alpha_j\n",
    "y^{(i)}\n",
    "y^{(j)}\n",
    "(\\mathbf{x}^{(i)})^\\intercal\n",
    "\\mathbf{x}^{(j)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Dobili smo **dualni optimizacijski problem**:\n",
    "\n",
    "> **Maksimizirati** izraz\n",
    "> \\begin{align}\n",
    " \\sum_{i=1}^N\\alpha_i - \n",
    " \\frac{1}{2}\\sum_{i=1}^N\n",
    " \\sum_{j=1}^N\n",
    " \\alpha_i\n",
    " \\alpha_j\n",
    " y^{(i)}\n",
    " y^{(j)}\n",
    " (\\mathbf{x}^{(i)})^\\intercal\n",
    " \\mathbf{x}^{(j)}\n",
    " \\end{align}\n",
    "> tako da:\n",
    "> \\begin{align*}\n",
    " \\alpha_i &\\geq 0,\\quad i=1,\\dots,N \\\\ \n",
    " \\sum_{i=1}^N\\alpha_i y^{(i)} &= 0\n",
    " \\end{align*}\n",
    "\n",
    "* Također vrijedi KKT-uvjet:\n",
    "$$\n",
    "\\alpha_i\\big(y^{(i)} h(\\mathbf{x}^{(i)})-1\\big) = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sažetak\n",
    "\n",
    "* Logistička regresija je **diskriminativan klasifikacijski model** s probabilističkim izlazom\n",
    "  \n",
    "  \n",
    "* Koristi se **logistička funkcija gubitka** odnosno **pogreška unakrsne entropije**\n",
    "\n",
    "\n",
    "* Optimizacija se provodi **gradijentnim spustom**, a prenaučenost se može spriječiti **regularizacijom**\n",
    "\n",
    "\n",
    "* Model **odgovara generativnom modelu** s normalno distribuiranim izglednostima i dijeljenom kovarijacijskom matricom, ali je broj parametara logističke regresije manji \n",
    "\n",
    "\n",
    "* Logistička regresija vrlo je dobar algoritam koji **nema        nedostatke** koje imaju klasifikacija regresijom i perceptron\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
