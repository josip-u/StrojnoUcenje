{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sveučilište u Zagrebu<br>\n",
    "Fakultet elektrotehnike i računarstva\n",
    "\n",
    "# Strojno učenje\n",
    "\n",
    "<a href=\"http://www.fer.unizg.hr/predmet/su\">http://www.fer.unizg.hr/predmet/su</a>\n",
    "\n",
    "Ak. god. 2015./2016.\n",
    "\n",
    "# Bilježnica 8: Stroj potpornih vektora (SVM)\n",
    "\n",
    "(c) 2015 Jan Šnajder\n",
    "\n",
    "<i>Verzija: 0.1 (2015-12-08)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sadržaj:\n",
    "\n",
    "* Uvod\n",
    "\n",
    "* Problem maksimalne margine\n",
    "\n",
    "* Metoda Lagrangeovih multiplikatora\n",
    "\n",
    "* Dualna formulacija problema maksimalne margine\n",
    "\n",
    "* Meka margina\n",
    "\n",
    "* Gubitak zglobnice\n",
    "\n",
    "* Jezgreni trik\n",
    "\n",
    "* Mercerove jezgre\n",
    "\n",
    "* Optimizacija hiperparametara\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uvod\n",
    "\n",
    "\n",
    "* Vrlo učinkovit **diskriminativan model**\n",
    "\n",
    "\n",
    "* Podsjetnik: Algoritam strojnog učenja definiran je \n",
    "  1. modelom\n",
    "  * pogreškom\n",
    "  * optimizacijskim postupkom\n",
    "    \n",
    "    \n",
    "* Model:\n",
    "    \n",
    "$$\n",
    "    h(\\mathbf{x}) = \\mathbf{w}^\\intercal\\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "\n",
    "* Dakle, to je poopćeni linearan model bez funkcije preslikavanja\n",
    "\n",
    "\n",
    "* Gornja definicija modela je tzv. **primarna formulacija**\n",
    "\n",
    "\n",
    "* Postoji i **dualna formulacija**:\n",
    "  * Umjesto da značajke novog primjera množimo težinama $\\mathbf{w}$, možemo za novi primjer izračunati koliko je sličan primjerima iz skupa za učenje i na temelju toga odrediti klasifikaciju\n",
    "  * Model efektivno postaje **neparametarski**!\n",
    "  \n",
    "  \n",
    "* U dualnoj formulaciji možemo iskoristiti tzv. **jezgreni trik**, koji nam omogućava jeftino preslikavanje primjera u prostor više dimenzije (a time i nelinearnost modela)\n",
    "  * SVM je jedna vrsta tzv. **jezgrenog stroja** (engl. *kernel machine*)\n",
    "\n",
    "\n",
    "* Model je jednostavan, no definicija pogreške i optimizacijskog postupka su nešto složeniji\n",
    "\n",
    "\n",
    "* Osnovna ideja: primjere dviju klasa razdvojiti tako da je prostor između njih što veći $\\Rightarrow$ tzv. **maksimalna margina**\n",
    "\n",
    "\n",
    "* Opravdanje: generalizacija će biti najbolja onda kada granicu između klasa povućemo točno po sredini margine\n",
    "\n",
    "\n",
    "* Do sada smo optimizacijski problem definirali tako da smo\n",
    "  * krenuli od log-izglednosti pa izveli MLE (Naivan bayes)\n",
    "  * krenuli od funkcije gubitka pa izveli funkciju pogreške (linearna regresija) i napravili analitičku minimizaciju\n",
    "  * krenuli od funkcije pogreške pa izveli funkciju gubitka (logistička regresija, perceptron) i iskoristili je za gradijentni spust\n",
    "  \n",
    "  \n",
    "* Kod SVM-a, krenut ćemo odmah od onoga što u konačnici želimo dobiti: maksimalnu marginu $\\Rightarrow$ to je ono što SVM optimizira\n",
    "\n",
    "\n",
    "* Naknadno ćemo iz toga izvesti funkciju gubitka (i funkciju pogreške), ali samo radi usporedbe s drugim algoritmima\n",
    "\n",
    "\n",
    "> **Sinopsis:**\n",
    "> \n",
    "> * Prvo ćemo se fokusirati na **linearan model** i **linearno odvojive probleme**\n",
    "  * Matematika: Lagrangeovi multiplikatori\n",
    "> \n",
    "> * Zatim ćemo proširiti **linearan model** tako da može raditi s **linearno neodvojivim problemima** (meka granica)\n",
    "> \n",
    "> * Na kraju ćemo proširiti na **nelinearan model** (jezgreni trik)\n",
    ">   * Matematika: Mercerove jezgre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem maksimalne margine\n",
    "\n",
    "* Model:\n",
    "$$\n",
    "h(\\mathbf{x}) = \\mathbf{w}^\\intercal x + w_0\n",
    "$$\n",
    "\n",
    "\n",
    "* Oznake primjera za učenje: $y\\in\\{-1,+1\\}$\n",
    "\n",
    "\n",
    "* Granica između klasa: hiperravnina $h(\\mathbf{x})=0$\n",
    "\n",
    "\n",
    "* Predikcija klase: $y=\\mathrm{sgn}(h(\\mathbf{x}))$\n",
    "\n",
    "\n",
    "* Pretpostavimo da su primjeri iz $\\mathcal{D}$ **linearno odvojivi**\n",
    "\n",
    "\n",
    "* Onda postoji $\\mathbf{w}$ i $w_0$ takvi da\n",
    "$$\n",
    "\\begin{align*}\n",
    "h(\\mathbf{x}^{(i)}) \\geq 0 & \\quad\\text{za $y^{(i)}=+1$}\\\\\n",
    "h(\\mathbf{x}^{(i)}) < 0 & \\quad\\text{za $y^{(i)}=-1$}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* Kraće, postoje $\\mathbf{w}$ i $w_0$ takvi da\n",
    "$$\n",
    "\\forall(\\mathbf{x}^{(i)},y^{(i)}) \\in D.\\ y^{(i)}h(\\mathbf{x}^{(i)})\\geq 0\n",
    "$$\n",
    "\n",
    "\n",
    "* Postoji beskonačno mnogo rješenja za $\\mathbf{w}$ i $w_0$ (prostor inačica je beskonačan)\n",
    "\n",
    "\n",
    "* No nas zanima rješenje **maksimalne margine** $\\Rightarrow$ induktivna pristranost preferencijom\n",
    "\n",
    "\n",
    "* **Margina = udaljenost hiperravnine do najbližeg primjera**\n",
    "\n",
    "\n",
    "* Ako maksimiziramo marginu, onda će hiperravnina prolaziti točno kroz na pola puta između dva primjera\n",
    "\n",
    "\n",
    "#### Formulacija optimizacijskog problema\n",
    "\n",
    "\n",
    "* Predznačena udaljenost primjera od hiperravnine je\n",
    "\n",
    "$$\n",
    "d = \\frac{h(\\mathbf{x})}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "\n",
    "\n",
    "* Nas zanimaju samo hiperravnine koje ispravno klasificiraju primjere. U tom slučaju **apsolutna** udaljenost primjera do hiperravnine je:\n",
    "\n",
    "$$\n",
    "\\frac{y^{(i)}h(\\mathbf{x})}{\\|\\mathbf{w}\\|} = \n",
    "\\frac{y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}^{(i)}+w_0)}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "\n",
    "\n",
    "* Po definiciji, margina je udaljenost hiperravnine do najbližeg primjera:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\|\\mathbf{w}\\|}\\mathrm{min}_i\\big\\{y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x} + w_0)\\big\\}\n",
    "$$\n",
    "\n",
    "\n",
    "* Tu udaljenost želimo maksimizirati:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmax}_{\\mathbf{w},w_0}\\Big\\{\\frac{1}{\\|\\mathbf{w}\\|}\\mathrm{min}_i\\big\\{y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x} + w_0)\\big\\}\\Big\\}\n",
    "$$\n",
    "\n",
    "\n",
    "* Ako je $\\mathcal{D}$ linearno odvojiv, onda postoji samo jedna takva margina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pojednostavljenje optimizacijskog problema\n",
    "\n",
    "\n",
    "* Gornji problem teško je riješiti izravno (min unutar max)\n",
    "\n",
    "\n",
    "* Vektor $(\\mathbf{w},w_0)$ možemo pomnožiti s proizvoljnom konstantom, a da to ne utječe na udaljenosti između primjera i hiperravnine:\n",
    "\n",
    "$$\n",
    "d=\\frac{h(\\mathbf{x})}{\\|\\mathbf{w}\\|}=\n",
    "\\frac{\\color{red}{\\alpha}\\mathbf{w}^\\intercal\\mathbf{x}+\\color{red}{\\alpha}w_0}{\\|\\color{red}{\\alpha}\\mathbf{w}\\|}=\n",
    "\\frac{\\color{red}{\\alpha}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0)}{\\color{red}{\\alpha}\\|\\mathbf{w}\\|}=\n",
    "\\frac{\\mathbf{w}^\\intercal\\mathbf{x}+w_0}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "\n",
    "\n",
    "* Kako bismo pojednostavili problem, možemo definirati da za primjer $\\mathbf{x}^{(i)}$ koji je najbliži margini vrijedi\n",
    "\n",
    "$$\n",
    "y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0)=1\n",
    "$$\n",
    "\n",
    "\n",
    "* Svi ostali primjeri jednako su blizu margine ili su od nje još udaljeniji:\n",
    "\n",
    "$$\n",
    "y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0) \\geq 1, \\qquad n=1,\\dots,N\n",
    "$$\n",
    "\n",
    "\n",
    "* Za primjere za koje $y^{(i)}h(\\mathbf{x})=1$ kažemo da su ograničenja **aktivna**, dok su za ostale primjere ograničenja neaktivna \n",
    "  * (Primjeri za koje su ograničenja aktivna zovemo potpornim vektorima, ali o tome više kasnije)\n",
    "\n",
    "\n",
    "* Uvijek će postojati barem **dva** aktivna ograničenja\n",
    "\n",
    "\n",
    "* [Skica: maksimalna margina]\n",
    "\n",
    "\n",
    "* Dakle, umjesto\n",
    "$$\n",
    "\\mathrm{argmax}_{\\mathbf{w},w_0}\\Big\\{\\frac{1}{\\|\\mathbf{w}\\|}\\underbrace{\\mathrm{min}_i\\big\\{y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x} + w_0)\\big\\}}_{=1}\\Big\\}\n",
    "$$\n",
    "mi sada maksimiziramo\n",
    "$$\n",
    "\\mathrm{argmax}_{\\mathbf{w},w_0}\\frac{1}{\\|\\mathbf{w}\\|}\n",
    "$$\n",
    "uz ograničenja\n",
    "$$\n",
    "y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0) \\geq 1, \\qquad n=1,\\dots,N\n",
    "$$\n",
    "\n",
    "\n",
    "* Maksimizator od $\\frac{1}{\\|\\mathbf{w}\\|}$ ekvivelentan je minimizatoru od $\\|\\mathbf{w}\\|=\\sqrt{\\mathbf{w}^\\intercal\\mathbf{w}}$, a taj je ekvivalentan minimizatoru od $\\|\\mathbf{w}\\|^2$. Još ćemo pomnožiti s $\\frac{1}{2}$ radi kasnije matematičke jednostavnosti\n",
    "\n",
    "\n",
    "* Konačna formulacija optimizacijskog problema\n",
    "\n",
    "> $\\mathrm{argmin}_{\\mathbf{w},w_0}\\frac{1}{2}\\|\\mathbf{w}\\|^2$\n",
    "\n",
    "> tako da $\\quad y^{(i)}(\\mathbf{w}^\\intercal\\mathbf{x}+w_0) \\geq 1, \\quad n=1,\\dots,N$\n",
    "\n",
    "\n",
    "* Naš optimizacijski problem sveo se na ciljnu funkciju koju želimo optimirati i ograničenja koja pritom moramo poštovati: tipičan problem **konveksne optimizacije uz ograničenja**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metoda Lagrangeovih multiplikatora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Problem **kvadratnog programiranja** (kvadratno ograničenog kvadratnog programiranja, QCQP)\n",
    "\n",
    "\n",
    "* Može se riješiti metodom **Langrangeovih multiplikatora**\n",
    "\n",
    "### Metoda Lagrangeovih multiplikatora\n",
    "\n",
    "\n",
    "### Lagrangeova dualnost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dualna formulacija problema maksimalne margine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Lagrangeova funkcija za naš problem:\n",
    "$$\n",
    "L(\\mathbf{w},w_0,\\color{red}{\\boldsymbol\\alpha})=\\frac{1}{2}\\|\\mathbf{w}\\|^2 -\n",
    "\\sum_{i=1}^N\\color{red}{\\alpha_i}\\Big\\{y^{(i)}\\big(\\mathbf{w}^\\intercal\\mathbf{x}^{(i)}+w_0\\big)-1\\Big\\}\n",
    "$$\n",
    "\n",
    "\n",
    "*  $\\color{red}{\\boldsymbol\\alpha=(\\alpha_1,\\dots,\\alpha_N)}$ je vektor Lagrangeovih multiplikatora, po jedan za svako ograničenje\n",
    "\n",
    "\n",
    "* Prelazimo na **dualnu formulaciju** problema jer je ta formulacija jednostavnija\n",
    "\n",
    "\n",
    "* Minimizator $(\\mathbf{w}^*, w_0^*)$: deriviranje po $\\mathbf{w}$ odnosno $w_0$ i izjednačavanje s nulom:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{w} &= \\sum_{i=1}^N \\alpha_i y^{(i)}\\mathbf{x}^{(i)}\\\\\n",
    "0 &= \\sum_{i=1}^N\\alpha_i y^{(i)}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dualna Lagrangeova funkcija:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{L}(\\boldsymbol\\alpha) &=\n",
    "\\frac{1}{2}\\|\\mathbf{w}\\|^2 -\\sum_{i=1}^N\\alpha_i\\Big\\{y^{(i)}\\big(\\mathbf{w}^\\intercal\\mathbf{x}^{(i)}+w_0\\big)-1\\Big\\}\\\\\n",
    "&= \n",
    "\\frac{1}{2}\\|\\mathbf{w}\\|^2\n",
    "-\\sum_{i=1}^N\\alpha_i y^{(i)}\\mathbf{w}^\\intercal\\mathbf{x}^{(i)}\n",
    "\\color{gray}{\\underbrace{-w_0 \\sum_{i=1}^N\\alpha_iy^{(i)}}_{=0}} + \n",
    "\\sum_{i=1}^N\\alpha_i\\\\\n",
    "&= \\nonumber\n",
    "\\frac{1}{2}\\sum_{i=1}^N\\alpha_i y^{(i)}(\\mathbf{x}^{(i)})^\\intercal\\sum_{j=1}^N\\alpha_j y^{(j)}\\mathbf{x}^{(j)}\n",
    "-\n",
    "\\sum_{i=1}^N\\alpha_i y^{(i)}(\\mathbf{x}^{(i)})^\\intercal\\sum_{j=1}^N\\alpha_j y^{(j)}\\mathbf{x}^{(j)}\n",
    "+ \\sum_{i=1}^N\\alpha_i\\\\\n",
    "&= \n",
    "\\sum_{i=1}^N\\alpha_i - \n",
    "\\frac{1}{2}\\sum_{i=1}^N\n",
    "\\sum_{j=1}^N\n",
    "\\alpha_i\n",
    "\\alpha_j\n",
    "y^{(i)}\n",
    "y^{(j)}\n",
    "(\\mathbf{x}^{(i)})^\\intercal\n",
    "\\mathbf{x}^{(j)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Dobili smo **dualni optimizacijski problem**:\n",
    "\n",
    "> **Maksimizirati** izraz\n",
    "> \\begin{align}\n",
    " \\sum_{i=1}^N\\alpha_i - \n",
    " \\frac{1}{2}\\sum_{i=1}^N\n",
    " \\sum_{j=1}^N\n",
    " \\alpha_i\n",
    " \\alpha_j\n",
    " y^{(i)}\n",
    " y^{(j)}\n",
    " (\\mathbf{x}^(i))^\\intercal\n",
    " (\\mathbf{x}^(j))\n",
    " \\end{align}\n",
    "> tako da:\n",
    "> \\begin{align*}\n",
    " \\alpha_i &\\geq 0,\\quad i=1,\\dots,N \\\\ \n",
    " \\sum_{i=1}^N\\alpha_i y^{(i)} &= 0\n",
    " \\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sažetak\n",
    "\n",
    "* Logistička regresija je **diskriminativan klasifikacijski model** s probabilističkim izlazom\n",
    "  \n",
    "  \n",
    "* Koristi se **logistička funkcija gubitka** odnosno **pogreška unakrsne entropije**\n",
    "\n",
    "\n",
    "* Optimizacija se provodi **gradijentnim spustom**, a prenaučenost se može spriječiti **regularizacijom**\n",
    "\n",
    "\n",
    "* Model **odgovara generativnom modelu** s normalno distribuiranim izglednostima i dijeljenom kovarijacijskom matricom, ali je broj parametara logističke regresije manji \n",
    "\n",
    "\n",
    "* Logistička regresija vrlo je dobar algoritam koji **nema        nedostatke** koje imaju klasifikacija regresijom i perceptron\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
