{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sveučilište u Zagrebu<br>\n",
    "Fakultet elektrotehnike i računarstva\n",
    "\n",
    "# Strojno učenje\n",
    "\n",
    "<a href=\"http://www.fer.unizg.hr/predmet/su\">http://www.fer.unizg.hr/predmet/su</a>\n",
    "\n",
    "Ak. god. 2015./2016.\n",
    "\n",
    "# Bilježnica 5: Regresija\n",
    "\n",
    "(c) 2015 Jan Šnajder\n",
    "\n",
    "<i>Verzija: 0.2 (2015-11-06)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sadržaj:\n",
    "\n",
    "* Uvod\n",
    "\n",
    "* Osnovni pojmovi\n",
    "\n",
    "* Model, funkcija gubitka i optimizacijski postupak\n",
    "\n",
    "* Postupak najmanjih kvadrata\n",
    "\n",
    "* Probabilistička interpretacija regresije\n",
    "\n",
    "* Poopćeni linearan model regresije\n",
    "\n",
    "* Odabir modela\n",
    "\n",
    "* Regularizirana regresija\n",
    "\n",
    "* Sažetak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Osnovni pojmovi\n",
    "\n",
    "* Označen skup podataka: $\\mathcal{D}=\\{(\\mathbf{x}^{(i)},y^{(i)})\\},\\quad \\mathbf{x}\\in\\mathbb{R}^n,\\quad y\\in\\mathbb{R}$\n",
    "\n",
    "\n",
    "* Hipoteza $h$ aproksimira nepoznatu  funkciju $f:\\mathbb{R}^n\\to\\mathbb{R}$\n",
    "\n",
    "\n",
    "* Idealno, $y^{(i)}=f(\\mathbf{x}^{(i)})$, ali zbog šuma: $$y^{(i)}=f(\\mathbf{x}^{(i)})+\\varepsilon$$\n",
    "\n",
    "\n",
    "* $\\mathbf{x}$ - **ulazna varijabla** (nezavisna, prediktorska)\n",
    "\n",
    "\n",
    "* $y$ - **izlazna varijabla** (zavisna, kriterijska)\n",
    "\n",
    "\n",
    "### Vrste regresije\n",
    "\n",
    "* Broj **ulaznih** (nezavisnih) varijabli:\n",
    "  * Univarijatna (jednostavna, jednostruka) regresija: $n=1$\n",
    "  * Multivarijatna (višestruka, multipla) regresija: $n>1$\n",
    "\n",
    "\n",
    "* Broj **izlaznih** (zavisnih) varijabli:\n",
    "  * Jednoizlazna regresija: $f(\\mathbf{x}) = y$\n",
    "  * Višeizlazna regresija: $f(\\mathbf{x})=\\mathbf{y}$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, funkcija gubitka i optimizacijski postupak\n",
    "\n",
    "\n",
    "### (1) Model\n",
    "\n",
    "* **Linearan model regresije**: $h$ je linearna funkcija <u>parametara</u>\n",
    "$\\mathbf{w} = (w_0,\\dots,w_n)$\n",
    "\n",
    "\n",
    "* Linearna regresija:\n",
    "    $$h(\\mathbf{x}|\\mathbf{w}) = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n$$\n",
    "\n",
    "\n",
    "* Polinomijalna regresija:\n",
    "    * Univarijatna polinomijalna: $$h(x|\\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\dots + w_d x^d\\quad (n=1)$$\n",
    "    * Multivarijatna polinomijalna: $$h(\\mathbf{x}|\\mathbf{w}) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2\\quad (n=2, d=2)$$\n",
    "      * Modelira međuovisnost značajki (*cross-terms* $x_1 x_2, \\dots$) \n",
    "\n",
    "\n",
    "* Općenite **bazne funkcije**:\n",
    "    $$h(\\mathbf{x}|\\mathbf{w}) = w_0 + w_1\\phi_1(\\mathbf{x}) + \\dots + w_m\\phi_m(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Funkcija gubitka (funkcija pogreške)\n",
    "\n",
    "* Kvadratni gubitak (engl. *quadratic loss*)\n",
    "\n",
    "$$\n",
    "L(y^{(i)},h(\\mathbf{x}^{(i)})) = \\big(y^{(i)}-h(\\mathbf{x}^{(i)})\\big)^2\n",
    "$$\n",
    "\n",
    "* Funkcija pogreške (proporcionalna s empirijskim očekivanjem gubitka):\n",
    "$$\n",
    "E(h|\\mathcal{D})=\\frac{1}{2}\n",
    "\\sum_{i=1}^N\\big(y^{(i)}-h(\\mathbf{x}^{(i)})\\big)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Optimizacijski postupak\n",
    "\n",
    "* Postupak **najmanjih kvadrata** (engl. *least squares*)\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_{\\mathbf{w}} E(\\mathbf{w}|\\mathcal{D})\n",
    "$$\n",
    "\n",
    "\n",
    "* Rješenje ovog optimizacijskog problema postoji u **zatvorenoj formi**\n",
    "\n",
    "\n",
    "# Postupak najmanjih kvadrata\n",
    "\n",
    "\n",
    "* Razmotrimo najprije linearnu regresiju:\n",
    "$$h(\\mathbf{x}|\\mathbf{w}) = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\sum_{i=1}^n w_i x_i + w_0$$\n",
    "\n",
    "\n",
    "* Izračun je jednostavniji ako pređemo u matrični račun\n",
    "  * Svaki vektor primjera $\\mathbf{x}^{(i)}$ proširujemo *dummy* značajkom $x^{(i)}_0 = 1$, pa je model onda:\n",
    "\n",
    "$$h(\\mathbf{x}|\\mathbf{w}) = \\mathbf{w}^\\intercal \\mathbf{x}$$\n",
    "\n",
    "\n",
    "* Skup primjera:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    "1 & x^{(1)}_1 & x^{(1)}_2 \\dots & x^{(1)}_n\\\\\n",
    "1 & x^{(2)}_1 & x^{(2)}_2 \\dots & x^{(2)}_n\\\\\n",
    "\\vdots\\\\\n",
    "1 & x^{(N)}_1 & x^{(N)}_2 \\dots & x^{(N)}_n\\\\\n",
    "\\end{pmatrix}_{N\\times (n+1)}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 & (\\mathbf{x}^{(1)})^\\intercal \\\\\n",
    "1 & (\\mathbf{x}^{(2)})^\\intercal \\\\\n",
    "\\vdots\\\\\n",
    "1 & (\\mathbf{x}^{(N)})^\\intercal \\\\\n",
    "1 & \\end{pmatrix}_{N\\times (n+1)}\n",
    "$$\n",
    "* Matricu primjera $\\mathbf{X}$ zovemo **dizajn-matrica**\n",
    "\n",
    "\n",
    "* Vektor izlaznih vrijednosti:\n",
    "$$\n",
    "\\mathbf{y} = \n",
    "\\begin{pmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "y^{(N)}\\\\\n",
    "\\end{pmatrix}_{N\\times 1}\n",
    "$$\n",
    "\n",
    "### Egzaktno rješenje\n",
    "\n",
    "* Idealno, tražimo egzaktno rješenje, tj. rješenje za koje vrijedi\n",
    "$$\n",
    "(\\mathbf{x}^{(i)}, y^{(i)})\\in\\mathcal{D}.\\ h(\\mathbf{x}^{(i)}) = y^{(i)}\n",
    "$$\n",
    "odnosno\n",
    "$$\n",
    "(\\mathbf{x}^{(i)}, y^{(i)})\\in\\mathcal{D}.\\ \\mathbf{w}^\\intercal \\mathbf{x} = y^{(i)}\n",
    "$$\n",
    "\n",
    "\n",
    "* Možemo napisati kao matričnu jednadžbu ($N$ jednadžbi s $(n+1)$ nepoznanica):\n",
    "\n",
    "$$\n",
    "\\mathbf{X}\\mathbf{w} = \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & x^{(1)}_1 & x^{(1)}_2 \\dots & x^{(1)}_n\\\\\n",
    "1 & x^{(2)}_1 & x^{(2)}_2 \\dots & x^{(2)}_n\\\\\n",
    "\\vdots\\\\\n",
    "1 & x^{(N)}_1 & x^{(N)}_2 \\dots & x^{(N)}_n\\\\\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "w_0\\\\\n",
    "w_1\\\\\n",
    "\\vdots\\\\\n",
    "w_n\\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "y^{(N)}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "* Egzaktno rješenje ovog sustava jednadžbi je\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{X}^{-1}\\mathbf{y}\n",
    "$$\n",
    "\n",
    "Međutim, rješenja <u>nema</u> ili ono <u>nije jedinstveno</u> ako:\n",
    "\n",
    "* (1) $\\mathbf{X}$ nije kvadratna, pa nema inverz. U pravilu:\n",
    "    * $N>(n+1)$ <br>\n",
    "      $\\Rightarrow$ sustav je **preodređen** (engl. *overdetermined*) i nema rješenja\n",
    "    * $N<(n+1)$ <br>\n",
    "      $\\Rightarrow$ sustav je **pododređen** (engl. *underdetermined*) i ima višestruka rješenja\n",
    "      \n",
    "* (2) $\\boldsymbol{X}$ jest kvadratna (tj. $N=(n+1)$), ali ipak nema inverz (ovisno o rangu matrice)<br> $\\Rightarrow$ sustav je **nekonzistentan**\n",
    "\n",
    "\n",
    "### Rješenje najmanjih kvadrata\n",
    "\n",
    "\n",
    "* <u>Približno</u> rješenje sustava $\\mathbf{X}\\mathbf{w}=\\mathbf{y}$\n",
    "\n",
    "\n",
    "* Funkcija pogreške:    \n",
    "$$\n",
    "E(\\mathbf{w}|\\mathcal{D})=\\frac{1}{2}\n",
    "\\sum_{i=1}^N\\big(\\mathbf{w}^\\intercal\\mathbf{x}^{(i)} - y^{(i)}\\big)^2\n",
    "$$\n",
    "\n",
    "\n",
    "* Matrični oblik:\n",
    "\\begin{align*}\n",
    "E(\\mathbf{w}|\\mathcal{D}) \n",
    "=& \n",
    "\\frac{1}{2} (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^\\intercal (\\mathbf{X}\\mathbf{w} - \\mathbf{y})\\\\\n",
    "=&\n",
    "\\frac{1}{2}\n",
    "(\\mathbf{w}^\\intercal\\mathbf{X}^\\intercal\\mathbf{X}\\mathbf{w} - \\mathbf{w}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y} - \\mathbf{y}^\\intercal\\mathbf{X}\\mathbf{w} + \\mathbf{y}^\\intercal\\mathbf{y})\\\\\n",
    "=&\n",
    "\\frac{1}{2}\n",
    "(\\mathbf{w}^\\intercal\\mathbf{X}^\\intercal\\mathbf{X}\\mathbf{w} - 2\\mathbf{y}^\\intercal\\mathbf{X}\\mathbf{w} + \\mathbf{y}^\\intercal\\mathbf{y})\n",
    "\\end{align*}\n",
    "\n",
    "> Jednakosti linearne algebre:\n",
    "> * $(A^\\intercal)^\\intercal = A$\n",
    "> * $(AB)^\\intercal = B^\\intercal A\\intercal$\n",
    "\n",
    "* Minimizacija pogreške:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mathbf{w}}E &= \n",
    "\\frac{1}{2}\\Big(\\mathbf{w}^\\intercal\\big(\\mathbf{X}^\\intercal\\mathbf{X}+(\\mathbf{X}^\\intercal\\mathbf{X})^\\intercal\\big) -\n",
    "2\\mathbf{y}^\\intercal\\mathbf{X}\\Big) = \n",
    "\\mathbf{X}^\\intercal\\mathbf{X}\\mathbf{w} - \\mathbf{X}^\\intercal\\mathbf{y} = \\mathbf{0}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "> Jednakosti linearne algebre:\n",
    "> * $\\frac{\\mathrm{d}}{\\mathrm{d}x}x^\\intercal A x=x^\\intercal(A+A^\\intercal)$\n",
    "> * $\\frac{\\mathrm{d}}{\\mathrm{d}x}A x=A$\n",
    "\n",
    "\n",
    "* Dobivamo sustav tzv. **normalnih jednadžbi**:\n",
    "$$\n",
    "\\mathbf{X}^\\intercal\\mathbf{X}\\mathbf{w} = \\mathbf{X}^\\intercal\\mathbf{y}\n",
    "$$\n",
    "\n",
    "\n",
    "* Rješenje:\n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{y} = \\color{red}{\\mathbf{X}^{+}}\\mathbf{y}\n",
    "$$\n",
    "\n",
    "\n",
    "* Matrica $\\mathbf{X}^{+}=(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal$ je **pseudoinverz** (Moore-Penroseov inverz) matrice $\\mathbf{X}$\n",
    "\n",
    "\n",
    "* Q: Kojih je dimenzija matrica $(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}$?\n",
    "* Q: Što utječe na složenost izračuna inverza matrice: broj primjera $N$ ili broj dimenzija $n$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistička interpretacija regresije"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ograničimo se BSO na univarijatnu ($n=1$) linearnu regresiju:\n",
    "\n",
    "$$\n",
    "h(x|w_0, w_1) = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "\n",
    "* Zbog šuma u $\\mathcal{D}$:\n",
    "$$\n",
    "    y^{(i)} = f(\\mathbf{x}^{(i)}) + \\color{red}{\\varepsilon}\n",
    "$$\n",
    "\n",
    "* Prepostavka:\n",
    "$$\n",
    "    \\color{red}{\\varepsilon}\\ \\sim\\ \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "* Posjedično:\n",
    "$$\n",
    "    \\color{red}{y|x}\\ \\sim\\        \\mathcal{N}\\big(f(x), \\sigma^2\\big)\n",
    "$$\n",
    "odnosno\n",
    "$$\n",
    "    \\color{red}{p(y|x)} = \\mathcal{N}\\big(f(x), \\sigma^2\\big)\n",
    "$$\n",
    "\n",
    "* Vrijedi \n",
    "$$\\mathbb{E}[y|x] = \\mu = f(x)$$\n",
    "\n",
    "\n",
    "* Naš cilj je: $h(\\mathbf{x}|\\mathbf{w}) = f(x)$\n",
    "\n",
    "\n",
    "* [Skica]\n",
    "\n",
    "\n",
    "* $p(y^{(i)}|x^{(i)})$ je vjerojatnost da je $f(x^{(i)})$ generirala vrijednost $y^{(i)}$\n",
    "  * (Formulacija nije baš točna, jer je $x$ kontinuirana varijabla a $p$ je gustoća vjerojatnosti.)\n",
    "  \n",
    "### Log-izglednost\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ln\\mathcal{L}(\\mathbf{w}|\\mathcal{D}) \n",
    "&= \n",
    "\\ln p(\\mathcal{D}|\\mathbf{w}) = \n",
    "\\ln\\prod_{i=1}^N p(x^{(i)}, y^{(i)}) =\n",
    "\\ln\\prod_{i=1}^N p(y^{(i)}|x^{(i)}) p(x^{(i)}) \\\\ \n",
    "&= \n",
    "\\ln\\prod_{i=1}^N p(y^{(i)}|x^{(i)}) + \\underbrace{\\color{gray}{\\ln\\prod_{i=1}^N p(x^{(i)})}}_{\\text{Ne ovisi o $\\mathbf{w}$}} \\\\\n",
    "& \\Rightarrow \\ln\\prod_{i=1}^N p(y^{(i)}|x^{(i)}) =\n",
    "\\ln\\prod_{i=1}^N\\mathcal{N}\\big(h(x^{(i)}|\\mathbf{w}),\\sigma^2\\big)\\\\ &= \n",
    "\\ln\\prod_{i=1}^N\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\Big\\{-\\frac{\\big(y^{(i)}-h(x^{(i)}|\\mathbf{w})\\big)^2}{2\\sigma^2}\\Big\\}\\\\ \n",
    "&=\n",
    "\\underbrace{\\color{gray}{-N\\ln(\\sqrt{2\\pi}\\sigma)}}_{\\text{konst.}} -\n",
    "\\frac{1}{2\\color{gray}{\\sigma^2}}\\sum_{i=1}^N\\big(y^{(i)}-h(x^{(i)}|\\mathbf{w})\\big)^2\\\\\n",
    "& \\Rightarrow\n",
    "-\\frac{1}{2}\\sum_{i=1}^N\\big(y^{(i)}-h(x^{(i)}|\\mathbf{w})\\big)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* Uz pretpostavku Gaussovog šuma, **maksimizacija izglednosti** odgovara **minimizaciji funkcije pogreške** definirane kao **zbroj kvadratnih odstupanja**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{argmax}_{\\mathbf{w}} \\ln\\mathcal{L}(\\mathbf{w}|\\mathcal{D}) &= \\mathrm{argmin}_{\\mathbf{w}} E(\\mathbf{w}|\\mathcal{D})\\\\\n",
    "E(h|\\mathcal{D}) &=\\frac{1}{2} \\sum_{i=1}^N\\big(y^{(i)}-h(x^{(i)}|\\mathbf{w})\\big)^2\\\\\n",
    "L\\big(y,h(x|\\mathbf{w})\\big)\\ &\\propto\\ \\big(y - h(x|\\mathbf{w})\\big)^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* $\\Rightarrow$ Probabilističko opravdanje za kvadratnu funkciju gubitka\n",
    "\n",
    "\n",
    "* Rješenje MLE jednako je rješenju koje daje postupak najmanjih kvadrata!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poopćeni linearan model regresije"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zanima nas poopćenje na $n>1$ koje obuhvaća sve multivarijatne linearne modele regresije: univarijatna regresija, linearna regresija, polinomijalna regresija, ...\n",
    "  * $h(\\mathbf{x}|\\mathbf{w}) = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n$\n",
    "  * $h(x|\\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\dots + w_d x^d$\n",
    "  * $h(\\mathbf{x}|\\mathbf{w}) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2$\n",
    "  * ...\n",
    "\n",
    "\n",
    "* Uvodimo fiksan skup **baznih funkcija** (nelinearne funkcije ulaznih varijabli):\n",
    "$$\n",
    "    \\{\\phi_1, \\phi_2, \\dots, \\phi_m\\}\n",
    "$$    \n",
    "gdje $\\phi_j:\\mathbb{R}^n\\to\\mathbb{R}$\n",
    "\n",
    "\n",
    "* Dogovorno: $\\phi_0(\\mathbf{x}) = 1$\n",
    "\n",
    "\n",
    "* Svaki vektor primjera u $n$-dimenzijskom originalnom ulaznom prostoru (engl. *input space*) $\\mathcal{X}$:\n",
    "$$\n",
    "\\mathbf{x} = (x_1, x_2, \\dots, x_n)\n",
    "$$\n",
    "preslikavamo u nov, $m$-dimenzijski prostor, tzv. **prostor značajki** (engl. *feature space*):\n",
    "$$\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}) = (\\phi_1(x_1), \\phi_2(x_2), \\dots, \\phi_m(x_n))\n",
    "$$\n",
    "\n",
    "\n",
    "* **Funkija preslikavanja** (vektor baznih funkcija)\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\phi}&:\\mathbb{R}^n\\to\\mathbb{R}^m:\\\\\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}) &= \\big(\\phi_0(\\mathbf{x}),\\dots,\\phi_m(\\mathbf{x})\\big)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* Poopćen linearan model:\n",
    "$$\n",
    "    h(\\mathbf{x}|\\mathbf{w}) = \\sum_{j=0}^m w_j\\phi_j(\\mathbf{x}) = \\mathbf{w}^\\intercal\\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "\n",
    "### Uobičajene funkcije preslikavanja\n",
    "\n",
    "\n",
    "* Linearna regresija:\n",
    "$$\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}) = (1,x_1,x_2,\\dots,x_n)\n",
    "$$\n",
    "\n",
    "\n",
    "* Univarijatna polinomijalna regresija: \n",
    "$$\n",
    "\\boldsymbol{\\phi}(x) = (1,x,x^2,\\dots,x^m)\n",
    "$$\n",
    "\n",
    "\n",
    "* Polinomijalna regresija drugog stupnja: \n",
    "$$\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}) = (1,x_1,x_2,x_1 x_2, x_1^2, x_2^2)\n",
    "$$\n",
    "\n",
    "\n",
    "* Gaussove bazne funkcije (RBF):\n",
    "$$\n",
    "\\phi_j(x) = \\exp\\Big\\{-\\frac{(x-\\mu_j)^2}{2\\sigma^2}\\Big\\}\n",
    "$$\n",
    "\n",
    "\n",
    "* [Skica: RBF] \n",
    "\n",
    "### Prostor značajki\n",
    "\n",
    "\n",
    "* **Funkcija preslikavanja značajki** $\\mathbf{\\phi} : \\mathbb{R}^n \\to \\mathbb{R}^m $ preslikava primjere iz $n$-dimenzijskog ulaznog prostora u $m$-dimenzijski prostor značajki\n",
    "\n",
    "\n",
    "* Tipično je $m>n$\n",
    "\n",
    "\n",
    "* Tada je funkcija koja je linearna u prostoru značajki **nelinearna u ulaznom prostoru**\n",
    "\n",
    "---\n",
    "\n",
    "* Primjer:\n",
    "  * $\\mathcal{X} = \\mathbb{R}$\n",
    "  * $n=1$, $m=3$\n",
    "  * $\\boldsymbol{\\phi} : \\mathbb{R} \\to \\mathbb{R}^3$\n",
    "  * $\\boldsymbol{\\phi}(x) = (1,x,x^2)$\n",
    "  * [Skica]\n",
    "    \n",
    "    \n",
    "* Sada možemo koristiti linearan model za nelinearne probleme\n",
    "\n",
    "\n",
    "* Imamo unificiran postupak, neovisno koju funkciju $\\boldsymbol{\\phi}$ odaberemo\n",
    "\n",
    "\n",
    "### Optimizacijski postupak\n",
    "\n",
    "* Ništa se ne mijenja u odnosu na ono što smo već izveli, samo umjesto $\\mathbf{X}$ imamo dizajn-matricu $\\boldsymbol{\\Phi}$\n",
    "\n",
    "\n",
    "* Dizajn-matrica:\n",
    "$$\n",
    "\\boldsymbol{\\Phi} = \n",
    "\\begin{pmatrix}\n",
    "1 & \\phi_1(\\mathbf{x}^{(1)}) & \\dots & \\phi_m(\\mathbf{x}^{(1)})\\\\\n",
    "1 & \\phi_1(\\mathbf{x}^{(2)}) & \\dots & \\phi_m(\\mathbf{x}^{(2)})\\\\\n",
    "\\vdots\\\\\n",
    "1 & \\phi_1(\\mathbf{x}^{(N)}) & \\dots & \\phi_m(\\mathbf{x}^{(N)})\\\\\n",
    "\\end{pmatrix}_{N\\times m}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{\\phi}(\\mathbf{x}^{(1)})^\\intercal \\\\\n",
    "\\mathbf{\\phi}(\\mathbf{x}^{(2)})^\\intercal \\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{\\phi}(\\mathbf{x}^{(N)})^\\intercal \\\\\n",
    "\\end{pmatrix}_{N\\times m}\n",
    "$$\n",
    "\n",
    "\n",
    "* Rješenje u smislu najmanjih kvadrata:\n",
    "$$\n",
    "\\mathbf{w} = (\\boldsymbol{\\Phi}^\\intercal\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\intercal\\mathbf{y} = \\color{red}{\\boldsymbol{\\Phi}^{+}}\\mathbf{y}\n",
    "$$\n",
    "gdje\n",
    "$$\n",
    "\\boldsymbol{\\Phi}^{+}=(\\boldsymbol{\\Phi}^\\intercal\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\intercal\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odabir modela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Poopćeni linearan model regresije ima jedan **hiperparametar**: funkciju preslikavanje $\\boldsymbol{\\phi}$\n",
    "\n",
    "\n",
    "* Alternativno, možemo reći da se radi o dva hiperparametra:\n",
    "  * izgled baznih funkcija $\\phi_j$\n",
    "  * broj baznih funkcija $m$ (dimenzija prostora značajki)\n",
    "\n",
    "\n",
    "* Hiperparametre treba namjestiti tako da odgovaraju podatcima, odnosno treba\n",
    "dobro **odabrati model**\n",
    "\n",
    "\n",
    "* U suprotnom model može biti **podnaučen** ili **prenaučen**\n",
    "\n",
    "\n",
    "* Ako model ima mnogo parametra, lako ga je prenaučiti\n",
    "\n",
    "\n",
    "* Sprečavanje prenaučenosti:\n",
    "  1. Koristiti više primjera za učenje\n",
    "  2. Odabrati model unakrsnom provjerom\n",
    "  3. **Regularizacija**\n",
    "  4. <span style=\"color:gray\">Bayesovska regresija (bayesovski odabir modela)  $\\Rightarrow$ nećemo raditi</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizirana regresija\n",
    "\n",
    "\n",
    "### Ideja\n",
    "\n",
    "* Opažanje: kod linearnih modela, što je model složeniji, to ima veće vrijednosti parametara $\\mathbf{w}$\n",
    "\n",
    "\n",
    "* Prenaučeni linearni modeli imaju:\n",
    "  * ukupno previše parametara (težina) i/ili\n",
    "  * prevelike vrijednosti pojedinačnih parametara\n",
    "\n",
    "\n",
    "* Ideja: **ograničiti rast vrijednosti parametara** kažnjavanjem hipoteza s visokim vrijednostima parametara\n",
    "\n",
    "\n",
    "* Time ostvarujemo **kompromis** između točnosti i jednostavnosti modela i to već **pri samom učenju** modela\n",
    "\n",
    "\n",
    "* Efektivno se **graničava složenost** modela i sprečava se prenaučenost\n",
    "\n",
    "\n",
    "* Cilj: što više parametara (težina) pritegnuti na nulu $\\Rightarrow$ **rijetki modeli** (engl. *sparse models*)\n",
    "\n",
    "\n",
    "* Rijetki modeli su:\n",
    "  * teži za prenaučiti\n",
    "  * računalno jednostavniji\n",
    "  * interpretabilniji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizacija\n",
    "\n",
    "* U funkciju pogreške (koju minimiziramo) ugrađujemo mjeru složenosti modela:\n",
    "\n",
    "$$\n",
    "   E' = \\textrm{empirijska pogreška} + \\color{red}{\\lambda\\times\\textrm{složenost modela}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    E'(\\mathbf{w}|\\mathcal{D}) = E(\\mathbf{w}|\\mathcal{D}) + \\underbrace{\\color{red}{\\lambda E_R(\\mathbf{w})}}_{\\text{reg. izraz}}\n",
    "$$\n",
    "\n",
    "* $\\lambda$ je **regularizacijski faktor**\n",
    "  * $\\lambda=0\\ \\Rightarrow$ neregularizirana funkcija pogreške\n",
    "  * Veća vrijednost regularizacijskog faktora $\\lambda$ uzrokuje smanjuje efektivne složenost modela\n",
    "\n",
    "\n",
    "* [Skica: Regularizirana regresija]\n",
    "\n",
    "\n",
    "* Općenit regularizacijski izraz: **p-norma vektora težina**\n",
    "$$\n",
    "    E_R(\\mathbf{w}) = \\|\\mathbf{w}\\|_p = \\frac{1}{2}\\Big(\\sum_{j=\\color{red}{1}}^m |w_j|^p\\Big)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "\n",
    "* L2-norma ($p=2$):\n",
    "$$\\|\\mathbf{w}\\|_2 = \\frac{1}{2}\\sqrt{\\sum_{j=\\color{red}{1}}^m w_j^2} = \\sqrt{\\mathbf{w}^\\intercal\\mathbf{w}}$$\n",
    "\n",
    "\n",
    "* L1-norma ($p=1$):\n",
    "$$\\|\\mathbf{w}\\|_1 = \\frac{1}{2}\\sum_{j=\\color{red}{1}}^m |w_j|$$\n",
    "\n",
    "\n",
    "* L0-norma ($p=0$):\n",
    "$$\\|\\mathbf{w}\\|_0 = \\frac{1}{2}\\sum_{j=\\color{red}{1}}^m \\mathbf{1}\\{w_j\\neq 0\\}$$\n",
    "\n",
    "\n",
    "* **NB:** Težina $w_0$ se ne regularizira\n",
    "  * Q: Zašto?\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizirani linearni model regresije\n",
    " \n",
    "* **L2-regularizacija** ili Tikhononova regularizacija $\\Rightarrow$ **Ridge regression**:\n",
    "$$\n",
    "E(\\mathbf{w}|\\mathcal{D})=\\frac{1}{2}\n",
    "\\sum_{i=1}^N\\big(\\mathbf{w}^\\intercal\\boldsymbol{\\phi}(\\mathbf{x}^{(i)}) - y^{(i)}\\big)^2\n",
    "+ \\color{red}{\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2_2}\n",
    "$$\n",
    "  * ima rješenje u zatvorenoj formi\n",
    "  \n",
    "\n",
    "* **L1-regularizacija** $\\Rightarrow$ **LASSO regularization** (engl. *least absolute shrinkage and selection operator*)\n",
    "$$\n",
    "E(\\mathbf{w}|\\mathcal{D})=\\frac{1}{2}\n",
    "\\sum_{i=1}^N\\big(\\mathbf{w}^\\intercal\\boldsymbol{\\phi}(\\mathbf{x}^{(i)}) - y^{(i)}\\big)^2\n",
    "+ \\color{red}{\\frac{\\lambda}{2}\\|\\mathbf{w}\\|_1}\n",
    "$$\n",
    "  * nema rješenje u zatvorenoj formi!\n",
    "\n",
    "\n",
    "* **L0-regularizacija**\n",
    "$$\n",
    "E(\\mathbf{w}|\\mathcal{D})=\\frac{1}{2}\n",
    "\\sum_{i=1}^N\\big(\\mathbf{w}^\\intercal\\mathbf{\\phi}(\\mathbf{x}^{(i)}) - y^{(i)}\\big)^2\n",
    "+ \\color{red}{\\frac{\\lambda}{2}\\sum_{j=1}^m\\mathbf{1}\\{w_j\\neq0\\}}\n",
    "$$\n",
    "  * NP-potpun problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-regularizacija\n",
    "\n",
    "* Linearna regresija s L2-regularizacijom ima rješenje u zatvorenoj formi:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E'(\\mathbf{w}|\\mathcal{D}) &= \\frac{1}{2}\n",
    "(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y})^\\intercal\n",
    "(\\boldsymbol{\\Phi}\\mathbf{w} - \\mathbf{y}) + \\color{red}{\\frac{\\lambda}{2}\\mathbf{w}^\\intercal\\mathbf{w}}\\\\\n",
    "&=\n",
    "\\frac{1}{2}\n",
    "(\\mathbf{w}^\\intercal\\boldsymbol{\\Phi}^\\intercal\\boldsymbol{\\Phi}\\mathbf{w} - 2\\mathbf{y}^\\intercal\\boldsymbol{\\Phi}\\mathbf{w} + \\mathbf{y}^\\intercal\\mathbf{y}\n",
    "+ \\color{red}{\\lambda\\mathbf{w}^\\intercal\\mathbf{w}})\\\\\n",
    "\\nabla_{\\mathbf{w}}E' &= \n",
    "\\boldsymbol{\\Phi}^\\intercal\\boldsymbol{\\Phi}\\mathbf{w} - \\boldsymbol{\\Phi}^\\intercal\\mathbf{y} + \\color{red}{\\lambda\\mathbf{w}} \\\\\n",
    "&=\n",
    "(\\boldsymbol{\\Phi}^\\intercal\\boldsymbol{\\Phi} + \\color{red}{\\lambda\\mathbf{I}})\\mathbf{w} - \\boldsymbol{\\Phi}^\\intercal\\mathbf{y} = 0 \\\\\n",
    "\\mathbf{w} &= (\\boldsymbol{\\Phi}^\\intercal\\boldsymbol{\\Phi} + \\color{red}{\\lambda\\mathbf{I}})^{-1}\\boldsymbol{\\Phi}^\\intercal\\mathbf{y}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Napomene\n",
    "\n",
    "* Iznos parametra $w_j$ odgovara važnosti značajke, a predznak upućuje na njezin utjecaj (pozitivan ili negativan) na izlaznu vrijednost\n",
    "\n",
    "\n",
    "* Regularizacija smanjuje složenost modela na način da prigušuje vrijednosti pojedinih značajki, odnosno efektivno ih izbacuje (kada $w_j\\to0$)\n",
    "  * Ako je model nelinearan, to znači smanjivanje nelinearnosti\n",
    "  \n",
    "* Težinu $w_0$ treba izuzeti iz regularizacijskog izraza (jer ona definira pomak) ili treba centrirati podatke tako da $\\overline{y}=0$, jer onda $w_0\\to0$\n",
    "\n",
    "\n",
    "* L2-regularizacija kažnjava težine proporcionalno njihovom iznosu (velike težine više, a manje težine manje) Teško će parametri biti pritegnuti baš na nulu. Zato **L2-regularizacija ne rezultira rijetkim modelima**\n",
    "\n",
    "\n",
    "* L1-regularizirana regresija rezultira rijetkim modelima, ali nema rješenja u zatvorenoj formi (međutim mogu se koristiti iterativni optimizacijski postupci\n",
    "\n",
    "\n",
    "* Regularizacija je korisna kod modela s puno parametara, jer je takve modele lako prenaučiti\n",
    "\n",
    "\n",
    "* Regularizacija smanjuje mogućnost prenaučenosti, ali ostaje problem odabira hiperparametra $\\lambda$\n",
    "  * Taj se odabir najčešće radi **unakrsnom provjerom**\n",
    "    \n",
    "    \n",
    "* Q: Koju optimalnu vrijednost za $\\lambda$ bismo dobili kada bismo optimizaciju radili na skupu za učenje?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sažetak\n",
    "\n",
    "\n",
    "* **Linearan model regresije** linearan je u parametrima\n",
    "\n",
    "\n",
    "* Parametri linearnog modela uz kvadratnu funkciju gubitka imaju rješenje u zatvorenoj formi u obliku **pseudoinverza dizajn-matrice**\n",
    "\n",
    "\n",
    "* ** Nelinearnost regresijske funkcije ostvaruje se uporabom nelinearnih **baznih funkcija** (preslikavanjem ulaznog prostora u prostor značajki\n",
    "\n",
    "\n",
    "* Uz pretpostavku normalno distribuiranog šuma, **MLE je istovjetan postupku najmanjih kvadrata**, što daje probabilističko opravdanje za uporabu kvadratne funkcije gubitka\n",
    "\n",
    "\n",
    "* **Regularizacija smanjuje prenaučenost** ugradnjom dodatnog izraza u funkciju pogreške kojim se kažnjava složenost modela\n",
    "\n",
    "\n",
    "* **L2-regularizirana regresija istovjetna je MAP-procjeni** parametara (uz pretpostavku Gaussovog šuma) te ima rješenje u zatvorenoj formi\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
